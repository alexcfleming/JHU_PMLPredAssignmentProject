---
title: "PMLProject"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(corrplot)
library(Hmisc)
library(rpart)
library(randomForest)
library(e1071)
library(gbm)
```

## PML Final Project

First we download the relevant files - these are sourced from the following work:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. “Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13)”. Stuttgart, Germany: ACM SIGCHI, 2013.

Our goal is to "predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases."

```{r files}
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url1, "train.csv", method="auto")
download.file(url2, "test.csv", method="auto")
training <- read.csv("train.csv")
testing <- read.csv("test.csv")
```

## Executive Summary

The data appears to have a large number of predictors that are largely zero or NA, we remove these in both the training and the test sets to speed up the processing and training. Using the Caret nearZeroVar function removes 60 columns, and then removing the columns with over 90% NA values gets us to 59 predictors. This seems to be a good net reduction. We also create an additional cross validation set with 80% of the training data. This leaves us with the 20 final prediction tests in the testing set.

```{r cleaning}
set.seed(123)
rmindex = nearZeroVar(training)
#remove near zero predictors
training2 = training[,-rmindex]
testing2 = testing[,-rmindex]
#remove columns where there are more than 90% NA values
training3 = training2[,-which(colMeans(is.na(training2))>.9)]
testing3 = testing2[, colnames(testing2) %in% colnames(training3)]
#This step also removes the problem id field from the final quiz testing set
#create the cross validation split from the training data
inTrain = createDataPartition(training3$classe, p=0.8, list=FALSE)
training4 = training3[inTrain,]
crossval4 = training3[-inTrain,]
dim(training4)
dim(crossval4)
dim(testing3)
```

We then create a Correlation Matrix while removing the non-numerical columns remaining to look at the relationships between the predictors. The factor variables are in columns 2, 5, and 59.The size of the training set makes a correlation matrix hard to visualize, so we use a heat map.

```{r explore}
training4num  = training4[,-c(2,5,59)]
col<- colorRampPalette(c("blue", "white", "red"))(20)
res = cor(training4num)
heatmap(x = res, col = col, symm = TRUE)
```

There is a high amount of correlation between clusters of variables. We now test and attempt cross validation with a few different types of models to predict the classe factor. We preprocess with center and scaling.

```{r model1}
set.seed(123)
conGBM = trainControl(method="repeatedcv", number=5, repeats=1)
mod1 = train(classe~., data=training4, method="gbm", trControl=conGBM, preProcess=c("center", "scale"), verbose=FALSE)
mod1$finalModel
predict1 = predict(mod1, crossval4)
confusionMatrix(crossval4$classe, predict1)
```

We see with a gradient boosting with trees an accuracy of 1 on the cross validation set, which seems like a pretty good start. Let's try a tree based model with preprocessing.

```{r model2}
set.seed(123)
mod2 = train(classe~., data=training4, method="rpart", preProcess=c("center", "scale"))
mod2$finalModel
predict2 = predict(mod2, crossval4)
confusionMatrix(crossval4$classe, predict2)
```

The basic tree model shows very interesting but much worse results, with accuracy on the cross validation set dropping to .6617. Let's attempt one more model with cross validation, and then run a final evaluation on the testing set.

```{r model3rf}
set.seed(123)
mod3 = train(classe~., data=training4, method="rf")
mod3$finalModel
predict3 = predict(mod3, crossval4)
confusionMatrix(crossval4$classe, predict3)
```

This is pretty imprssive, an accuracy of 1, which means perfect results in the cross validation set, which makes me concerned about potential overfitting. We now use our best model, which was the RandomForest application, to do the prediction on the test cases and get a final accuracy.

We summarize our 3 results for training and cross validation:
GBM - Accuracy: 100% Sensitivity: 1 Specificity: 1
Rpart Tree Based Model - Accuracy: 66%  Sensitivity: .35 (lowest) Specificity: .82 (lowest)
Random Forest Model - Accuracy: 100% Sensitivity: 1 Specificity: 1

Therefore we use the random forest model to predict on the final test set provided, though we could easily use the GBM model as well:

```{r testpred}
predictfinal = predict(mod3, newdata=testing3, type="raw")
predictfinal
```

My final prediction here is very confusing, since my results on the final testing set do not match the quiz answers. I will submit in the quiz, however, since all else seems correct here with the results for the previous training. I have beaten my head on this for days and cannot figure out what I am doing wrong.

I look forward to the comments of my classmates.

Alex Fleming
December 28th, 2018